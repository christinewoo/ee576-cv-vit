\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{ViT Pokemon Classifier}


\author{
Christine Wu\\
Department of Electrical and Computer Engineering\\
University of Washington\\
Seattle, WA 98105 \\
\texttt{wuc29@uw.edu} \\
\And
Pei-Hsuan Lin \\
Department of Electrical and Computer Engineering\\
University of Washington\\
Seattle, WA 98105 \\
\texttt{peihslin@uw.edu} \\
\AND
Zhiwei Zhong \\
Department of Electrical and Computer Engineering\\
University of Washington\\
Seattle, WA 98105 \\
\texttt{zhongz22@uw.edu} \\
\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
In this report, we present a comprehensive study on the application of Vision Transformers 
(ViTs) for the task of Pokemon classification. While convolutional neural networks (CNNs) 
have been widely used, the ViT has recently emerged as a promising alternative due to the 
self-attention mechanisms. In this project, we developed a ViT model from scratch to explore
 the functionality of the ViT. The ViT model captures spatial relationships using 
 self-attention mechanisms, enabling it to learn both local and global features. 
 In addition, we fine-tuned a pre-trained model, which was pre-trained on ImageNet-21k at 
 resolution 224x224, to increase the accuracy of our model. Lastly, we discussed the 
 performance of the model that we developed from scratch and the pre-trained model.
\end{abstract}

\section{Related work-Pei-Hsuan}

NIPS requires electronic submissions.  The electronic submission site is  
\begin{center}
   \url{http://papers.nips.cc}
\end{center}

Please read carefully the
instructions below, and follow them faithfully.
\subsection{Style}

Papers to be submitted to NIPS 2015 must be prepared according to the
instructions presented here. Papers may be only up to eight pages long,
including figures. Since 2009 an additional ninth page \textit{containing only
cited references} is allowed. Papers that exceed nine pages will not be
reviewed, or in any other way considered for presentation at the conference.
%This is a strict upper bound. 

Please note that this year we have introduced automatic line number generation
into the style file (for \LaTeXe and Word versions). This is to help reviewers
refer to specific lines of the paper when they make their comments. Please do
NOT refer to these line numbers in your paper as they will be removed from the
style file for the final version of accepted papers.

The margins in 2015 are the same as since 2007, which allow for $\approx 15\%$
more words in the paper compared to earlier years. We are also again using 
double-blind reviewing. Both of these require the use of new style files.

Authors are required to use the NIPS \LaTeX{} style files obtainable at the
NIPS website as indicated below. Please make sure you use the current files and
not previous versions. Tweaking the style files may be grounds for rejection.

%% \subsection{Double-blind reviewing}
\subsection{Retrieval of style files}

The style files for NIPS and other conference information are available on the World Wide Web at
\begin{center}
   \url{http://www.nips.cc/}
\end{center}
The file \verb+nips2015.pdf+ contains these 
instructions and illustrates the
various formatting requirements your NIPS paper must satisfy. \LaTeX{}
users can choose between two style files:
\verb+nips15submit_09.sty+ (to be used with \LaTeX{} version 2.09) and
\verb+nips15submit_e.sty+ (to be used with \LaTeX{}2e). The file
\verb+nips2015.tex+ may be used as a ``shell'' for writing your paper. All you
have to do is replace the author, title, abstract, and text of the paper with
your own. The file
\verb+nips2015.rtf+ is provided as a shell for MS Word users.

The formatting instructions contained in these style files are summarized in
sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

\section{Technical Description-Ted and Christine, DataLoader-Pei-Hsuan}
\label{gen_inst}

% ALGORITHM
Our goal of this project is to learn the ViT architecture, implement from scratch, and
perform image classification on 150 different Pokemon. Below, we describe the individual 
moduals in our self-implemented ViT.

\subsection{Algorithm by Step}

1. This module takes in a 2D image with shape HWC and flattens the image into 2D patches each with shape $N * (P * P * C)$.
Here $N = {H * W} / {P^2}$, which is the number of patches, and $P$ is the number of pixels for each image patch.

2. We map the flattened patches to a dimension $D$, where $D$ is the constant latent vector size throughout the transformer layers.
These traininable linear projection outputs are the patch embeddings.
We prepend a class token in front of each patch embedding to serve as the image representation that will later be used in the
classification head at the end of the model. Lastly, to retain the positional information of each patch relative to the original image,
a 1D learnable positional embeddings are added.

3. The transformer block, shown below in Fig. 2, takes embedded patches as inputs and passes them a multi-headed self-attention layer (MSA)
followed by a MLP block with two layers and a GELU non-linearity layer. Before the MSA and MLP blocks, a block of Layernorm is added.
Additionally, there are residual connections after every block.

4. The multi-head self-attention module takes in the embedded sequence of tokens and computes the global self-attention.
For each token, it is mapped to $v$, $k$, and $q$ and passed into the scaled dot-product attention head.
The scaled dot product is used to compare the "query" with the "keys" and get scores for the "values."
Each of the scores is in short the relevance between the "query" and each "key".
We reweight the "values" with the obtained scores and take the summation of the reweighted "values".
Lastly, it is concatenated and passed into another linear layer for information flow, which helps to mix the information captured by each head.

5. The classification head is implemented with a MLP consisting of one hidden layer during pre-train.
During the fine-tuning stage, the MLP will consist of one single linear layer.



% Pre-train
\subsection{Pretrained Model}
For Transformers, and other self-attention-based architectures, the training usually consists of pre-training on 
a large dataset, such as a large text corpus for natural language processing tasks, and then fine-tune on a 
task-specific dataset. In our case, we want to perform image classification, therefore we used the pre-trained weights
released on Hugging Face. The pretrained weight we have used is "vit-base-patch16-224". It is a ViT model pre-trained
on ImageNet-21k, which consists of 14 millioin images and 21,843 classes. Each of the image has resolution 224x224.
Through this pre-training, the model learns the representation of images that can be used to extract features for the
fine-tune task. The fine-tune training trains a Pokemon classifier by adding a linear layer on top of the pre-trained
ViT encoder for the 150 classes in the Pokemon dataset.

\subsection{Dataloader} % include dataset information

%The version of the paper submitted for review should have ``Anonymous Author(s)'' as the author of the paper.

For the final version, authors' names are
set in boldface, and each name is centered above the corresponding
address. The lead author's name is to be listed first (left-most), and
the co-authors' names (if different address) are set to follow. If
there is only one co-author, list both author and co-author side by side.

Please pay special attention to the instructions in section \ref{others}
regarding figures, tables, acknowledgments, and references.

\section{Experimental results-Ted and Christine}
\label{headings}
We train our from-scratch ViT model on a GTX3060 GPU with the CIFAR-10 dataset. There are 50,000 RGB training images
with resolution 32x32. The use a batch size of 32, learning rate of 0.0002. The number of hidden dimension is 128,
number of heads is 4, and the number of transformer blocks is 4. We train the model for 30 epochs. Our PokemonClassifier
that utilizes the pre-trained weights on ImageNet-21K was trained on a GTX3060 GPU, with batch size of 16 and learning 
rate of $5 * 10^{-5}$ for 5 epochs. 

We present our experimental results for the from-scratch model and pretrained model, respectively:

% \subsection{Experimental Results for Model from Scratch}

\begin{table}[ht]
  \centering
    \begin{tabular}{ c | c | c | c | c  | c | c }
      \hline
      Image Size & Learning Rates & Hidden Dimension & Heads & Number of Blocks & Epoch & Accuracy \\ \hline
      3 x 32 x 32 & 0.0002 & 128 & 4 & 4 & 30 & 62.64\% \\ \hline
      3 x 32 x 32 & 0.001 & 64 & 2 & 6 & 20 & 57.94\% \\ \hline
    \end{tabular}
    \caption{Best Training Results of the Model from Scratch}
  \label{tab:my_label}
\end{table}


% \subsubsection{Experimental Results for Pretrained Model}
\begin{table}[ht]
  \centering
    \begin{tabular}{ c | c | c | c | c  | c }
      \hline
      Item & Epoch 1 & Epoch 2 & Epoch 3 & Epoch 4 & Epoch 5 \\ \hline
      validation loss & 2.62 & 0.9648 & 0.5622 & 0.4275 & 0.371\\ \hline
      validation accuracy & 0.7287  & 0.9124 & 0.9432 & 0.952 & 0.952 \\ \hline
    \end{tabular}
    \caption{Training Results of the Pretrained Model}
  \label{tab:my_label}
\end{table}

\section{Discussion of results-All}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Future works-Christine}


Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PostScript or PDF files}

Please prepare PostScript or PDF files with paper size ``US Letter'', and
not, for example, ``A4''. The -t
letter option on dvips will produce US Letter files.

Fonts were the main cause of problems in the past years. Your PDF file must
only contain Type 1 or Embedded TrueType fonts. Here are a few instructions
to achieve this.

\begin{itemize}

\item You can check which fonts a PDF files uses.  In Acrobat Reader,
select the menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
available out-of-the-box on most Linux machines.

\item The IEEE has recommendations for generating PDF files whose fonts
are also acceptable for NIPS. Please see
\url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

\item LaTeX users:

\begin{itemize}

\item Consider directly generating PDF files using \verb+pdflatex+
(especially if you are a MiKTeX user). 
PDF figures must be substituted for EPS figures, however.

\item Otherwise, please generate your PostScript and PDF files with the following commands:
\begin{verbatim} 
dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
ps2pdf mypaper.ps mypaper.pdf
\end{verbatim}

Check that the PDF files only contains Type 1 fonts. 
%For the final version, please send us both the Postscript file and
%the PDF file. 

\item xfig "patterned" shapes are implemented with 
bitmap fonts.  Use "solid" shapes instead. 
\item The \verb+\bbold+ package almost always uses bitmap
fonts.  You can try the equivalent AMS Fonts with command
\begin{verbatim}
\usepackage[psamsfonts]{amssymb}
\end{verbatim}
 or use the following workaround for reals, natural and complex: 
\begin{verbatim}
\newcommand{\RR}{I\!\!R} %real numbers
\newcommand{\Nat}{I\!\!N} %natural numbers 
\newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}

\item Sometimes the problematic fonts are used in figures
included in LaTeX files. The ghostscript program \verb+eps2eps+ is the simplest
way to clean such figures. For black and white figures, slightly better
results can be achieved with program \verb+potrace+.
\end{itemize}
\item MSWord and Windows users (via PDF file):
\begin{itemize}
\item Install the Microsoft Save as PDF Office 2007 Add-in from
\url{http://www.microsoft.com/downloads/details.aspx?displaylang=en\&familyid=4d951911-3e7e-4ae6-b059-a2e79ed87041}
\item Select ``Save or Publish to PDF'' from the Office or File menu
\end{itemize}
\item MSWord and Mac OS X users (via PDF file):
\begin{itemize}
\item From the print menu, click the PDF drop-down box, and select ``Save
as PDF...''
\end{itemize}
\item MSWord and Windows users (via PS file):
\begin{itemize}
\item To create a new printer
on your computer, install the AdobePS printer driver and the Adobe Distiller PPD file from
\url{http://www.adobe.com/support/downloads/detail.jsp?ftpID=204} {\it Note:} You must reboot your PC after installing the
AdobePS driver for it to take effect.
\item To produce the ps file, select ``Print'' from the MS app, choose
the installed AdobePS printer, click on ``Properties'', click on ``Advanced.''
\item Set ``TrueType Font'' to be ``Download as Softfont''
\item Open the ``PostScript Options'' folder
\item Select ``PostScript Output Option'' to be ``Optimize for Portability''
\item Select ``TrueType Font Download Option'' to be ``Outline''
\item Select ``Send PostScript Error Handler'' to be ``No''
\item Click ``OK'' three times, print your file.
\item Now, use Adobe Acrobat Distiller or ps2pdf to create a PDF file from
the PS file. In Acrobat, check the option ``Embed all fonts'' if
applicable.
\end{itemize}

\end{itemize}
If your file contains Type 3 fonts or non embedded TrueType fonts, we will
ask you to fix it. 

\subsection{Margins in LaTeX}
 
Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+
from the graphicx package. Always specify the figure width as a multiple of
the line width as in the example below using .eps graphics
\begin{verbatim}
   \usepackage[dvips]{graphicx} ... 
   \includegraphics[width=0.8\linewidth]{myfile.eps} 
\end{verbatim}
or % Apr 2009 addition
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ... 
   \includegraphics[width=0.8\linewidth]{myfile.pdf} 
\end{verbatim}
for .pdf graphics. 
See section 4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps}) 
 
A number of width problems arise when LaTeX cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command.


\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include 
acknowledgments in the anonymized submission, only in the 
final paper. 

\subsubsection*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}